{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prith189/GLG_DL/blob/main/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "AEUX4mLXajLJ"
      },
      "outputs": [],
      "source": [
        "#Huggingface library has pretrained models that have been trained on a large corpus and can perform NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwbpk17iagFK",
        "outputId": "f31d9b22-566f-4e7a-f635-f5be831ad2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.96)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-rYDcBu4jeF",
        "outputId": "11cd8bfe-1fb4-4eaa-bd04-b9414ef7928e"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from pandas.core.groupby import groupby\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoConfig, TFAutoModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, multilabel_confusion_matrix\n",
        "from gensim.summarization.textcleaner import split_sentences"
      ],
      "metadata": {
        "id": "XxnHwXWs4lnU"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "FINETUNED_MODEL_NAME = 'finetuned_' + PRETRAINED_MODEL_NAME\n",
        "FILE_DIR = '/content/drive/My Drive/fourthbrain/NER_Labels'\n",
        "SEQUENCE_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "RUN_TRAINING = False"
      ],
      "metadata": {
        "id": "XkKdrCjj4eDO"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "DkhP1_wOuRtp"
      },
      "outputs": [],
      "source": [
        "class NER_Model:\n",
        "    def __init__(self):\n",
        "        #Initialize the pretrained model\n",
        "        self.config = AutoConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "        self.backbone = TFAutoModel.from_pretrained(PRETRAINED_MODEL_NAME,config=self.config)\n",
        "    \n",
        "    def build_model(self, num_classes, use_finetuned=False):\n",
        "        tokens = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), name = 'tokens', dtype=tf.int32)\n",
        "        att_masks = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), name = 'attention', dtype=tf.int32)\n",
        "        \n",
        "        features = self.backbone(tokens, attention_mask=att_masks)[0]\n",
        "        \n",
        "        target = tf.keras.layers.Dropout(0.5)(features)\n",
        "        target = tf.keras.layers.Dense(num_classes, activation='softmax')(target)\n",
        "        \n",
        "        self.model = tf.keras.Model([tokens,att_masks],target)\n",
        "\n",
        "        self.model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                           loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "                           metrics=['accuracy'])\n",
        "        if(use_finetuned):\n",
        "            self.model.load_weights(os.path.join(FILE_DIR, FINETUNED_MODEL_NAME))\n",
        "\n",
        "    def train_model(self, x_data_in, x_data_att, y_data, x_data_in_val, x_data_att_val, y_data_val):\n",
        "        history = self.model.fit(x = [x_data_in, x_data_att], y = y_data, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=([x_data_in_val, x_data_att_val], y_data_val))\n",
        "\n",
        "    def save_model(self):\n",
        "        self.model.save_pretrained(os.path.join(FILE_DIR, FINETUNED_MODEL_NAME))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "9dFYs1RMZTaH"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "class NERDataset:\n",
        "    def __init__(self):\n",
        "        #Read the data files\n",
        "        self.dataset = pd.read_csv(os.path.join(FILE_DIR, 'ner_dataset.csv'), encoding = 'ISO-8859-1')\n",
        "\n",
        "        #Preprocess the dataset\n",
        "        self.dataset[\"Sentence #\"] = self.dataset[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "\n",
        "        #Convert tags into labels using label encoder\n",
        "        self.tag_encoder = preprocessing.LabelEncoder()\n",
        "        self.dataset.loc[:, 'Tag'] = self.tag_encoder.fit_transform(self.dataset['Tag'])\n",
        "        self.background_class = self.tag_encoder.transform(['O'])[0]\n",
        "\n",
        "        self.sentences = self.dataset.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "        self.tags = self.dataset.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        " \n",
        "    def build_ner_dataset(self, mode='train'):\n",
        "        \n",
        "        #Initialize the tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME,normalization=True)\n",
        "        self.config = AutoConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "        self.token_type_ids = []\n",
        "        for sentence in self.sentences:\n",
        "            encoded = self.tokenizer.encode_plus(sentence,\n",
        "                                       add_special_tokens = True,\n",
        "                                       max_length = SEQUENCE_LENGTH,\n",
        "                                       is_split_into_words=True,\n",
        "                                       return_attention_mask=True,\n",
        "                                       padding = 'max_length',\n",
        "                                       truncation=True,return_tensors = 'np')\n",
        "            self.input_ids.append(encoded['input_ids'])\n",
        "            self.attention_masks.append(encoded['attention_mask'])\n",
        "            self.token_type_ids.append(encoded.word_ids())\n",
        "            #print('Length of sentence:{}, Length of encoded:{}'.format(len(sentence), encoded['input_ids'].shape))\n",
        "        self.input_ids = np.vstack(self.input_ids)\n",
        "        self.attention_masks = np.vstack(self.attention_masks)\n",
        "        self.token_type_ids = np.vstack(self.token_type_ids)\n",
        "\n",
        "        \n",
        "\n",
        "        self.tags_proper = []\n",
        "        for ntag, tag in enumerate(self.tags):\n",
        "            word_ids = self.token_type_ids[ntag][self.token_type_ids[ntag] != np.array(None)]\n",
        "            tag_proper = [tag[i] for i in word_ids]\n",
        "            self.tags_proper.append(tag_proper)\n",
        "        \n",
        "        self.targets = np.ones([self.input_ids.shape[0], SEQUENCE_LENGTH], dtype=np.int32)*self.background_class\n",
        "        for n, tag in enumerate(self.tags_proper):\n",
        "            tag_len = len(tag)\n",
        "            self.targets[n,1:tag_len+1] = np.array(tag)\n",
        "    \n",
        "    \n",
        "    def test_train_split(self):\n",
        "        self.seq_train, self.seq_test, self.mask_train, self.mask_test, self.target_train, self.target_test, self.word_id_train, self.word_id_test = train_test_split(self.input_ids, self.attention_masks, self.targets, self.token_type_ids, test_size=0.20, random_state=42)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the dataset\n",
        "dataset = NERDataset()\n",
        "dataset.build_ner_dataset()\n",
        "dataset.test_train_split()\n",
        "n_classes = dataset.tag_encoder.classes_.shape[0]"
      ],
      "metadata": {
        "id": "LO-k0H5V-dV2"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the model\n",
        "ner_modeler = NER_Model()\n",
        "if(RUN_TRAINING):\n",
        "    ner_modeler.build_model(n_classes)\n",
        "    ner_modeler.train_model(dataset.seq_train, dataset.mask_train, dataset.target_train, dataset.seq_test, dataset.mask_test, dataset.target_test)\n",
        "    ner_modeler.model.save_weights(os.path.join(FILE_DIR, FINETUNED_MODEL_NAME))\n",
        "else:\n",
        "    ner_modeler.build_model(n_classes,True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNqxeeVTKBPV",
        "outputId": "c0f4cb94-3db1-4037-b7fd-0416ee38ee1a"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "test_preds = ner_modeler.model.predict([dataset.seq_test, dataset.mask_test])\n",
        "test_preds = np.argmax(test_preds, axis=2)\n",
        "test_preds_flat = test_preds.flatten()\n",
        "test_true_flat = dataset.target_test.flatten()"
      ],
      "metadata": {
        "id": "2peM64jtvwd6"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('F1 Score when background class is included')\n",
        "print('Precision:{}, Recall:{}, F1 Score:{}'.format(precision_score(test_true_flat, test_preds_flat, average='micro'), recall_score(test_true_flat, test_preds_flat, average='micro'), f1_score(test_true_flat, test_preds_flat, average='micro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiktCaOdxGQM",
        "outputId": "750cefa6-d2aa-4c4e-f75a-37153e88090e"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score when background class is included\n",
            "Precision:0.9922868666597164, Recall:0.9922868666597164, F1 Score:0.9922868666597164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('F1 Score when background class is excluded')\n",
        "print('Precision:{}, Recall:{}, F1 Score:{}'.format(precision_score(test_true_flat, test_preds_flat, labels=np.arange(0,16), average='micro'), recall_score(test_true_flat, test_preds_flat, labels=np.arange(0,16), average='micro'), f1_score(test_true_flat, test_preds_flat, labels=np.arange(0,16), average='micro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjWkCVTt5phX",
        "outputId": "98e06558-6952-49ea-a287-aaa664b2a94f"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score when background class is excluded\n",
            "Precision:0.8387444459108706, Recall:0.8340661023251744, F1 Score:0.8363987321642045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to test a new sample text\n",
        "def test_new_text(sample_text):\n",
        "    #Tokenize the sample text, and get the word ids\n",
        "    encoded = dataset.tokenizer.encode_plus(sample_text,\n",
        "                                        add_special_tokens = True,\n",
        "                                        max_length = SEQUENCE_LENGTH,\n",
        "                                        is_split_into_words=True,\n",
        "                                        return_attention_mask=True,\n",
        "                                        padding = 'max_length',\n",
        "                                        truncation=True,return_tensors = 'np')\n",
        "    input_seq = encoded['input_ids']\n",
        "    att_mask = encoded['attention_mask']\n",
        "    word_ids = encoded.word_ids()\n",
        "\n",
        "    #Predict the classes for each token\n",
        "    sample_out = ner_modeler.model.predict([input_seq, att_mask])\n",
        "    sample_out = np.argmax(sample_out, axis=2)\n",
        "    word_ids = np.array(word_ids)\n",
        "    valid_sample_out = sample_out[0, word_ids!=None]\n",
        "    valid_word_ids = word_ids[word_ids!=None]\n",
        "    names = [sample_text[i] for i in valid_word_ids[valid_sample_out!=dataset.background_class]]\n",
        "    labels = [dataset.tag_encoder.inverse_transform([i])[0] for i in valid_sample_out[valid_sample_out!=dataset.background_class]]\n",
        "\n",
        "    #Combine the tokens and correponding labels. Output the final names and their corresponding classes\n",
        "    full_names = []\n",
        "    full_labels = []\n",
        "    prev_index = -1\n",
        "    completed = {}\n",
        "    for name, label in zip(names, labels):\n",
        "        if(name not in completed):\n",
        "            if(label[0]=='B'):\n",
        "                full_names.append(name)\n",
        "                full_labels.append(label[2:])\n",
        "                prev_index += 1\n",
        "            else:\n",
        "                full_names[prev_index] = full_names[prev_index] + ' ' + name\n",
        "            completed[name] = 1\n",
        "    return full_names, full_labels"
      ],
      "metadata": {
        "id": "XJ_lsY_w8bJi"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = dataset.sentences[-40000]\n",
        "print(\" \".join(sample_text))\n",
        "print(test_new_text(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tS8fa3v9aqt",
        "outputId": "9ee0d46b-d6b1-4141-e31d-b7fc2aff9c36"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The U.S. Senate passed the bill Thursday , after a new report showed the number of unemployed Americans signing up for benefits for the first time grew to the highest level in 16 years .\n",
            "(['U.S. Senate', 'Thursday', 'Americans', '16'], ['org', 'tim', 'gpe', 'tim'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "KLXymKxANJqw"
      },
      "outputs": [],
      "source": [
        "csv_file = '/content/drive/My Drive/fourthbrain/all-the-news-2-1.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "xB69ozhDN0rm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "def display_ner(doc):\n",
        "    spacy.displacy.render(doc, style=\"ent\",manual=True, jupyter=True)\n",
        "\n",
        "class NewsDataset:\n",
        "    def __init__(self):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.preprocess()\n",
        "        self.ner = None\n",
        "    \n",
        "    def preprocess(self):\n",
        "        self.df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1','date','year','month','day','title','publication'], inplace=True)\n",
        "        self.df = self.df.iloc[:10000,:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news = NewsDataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-GZ441kOuZq",
        "outputId": "e1952fc4-a15c-48b5-f75d-2bdfca2c89dd"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (1,3,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the end-to-end pipeline and test on new dataset\n",
        "class NER_Pipeline:\n",
        "    def __init__(self):\n",
        "        #Load the tokenizer in the dataset\n",
        "        self.dataset = NERDataset()\n",
        "        self.dataset.build_ner_dataset()\n",
        "        self.dataset.test_train_split()\n",
        "        n_classes = self.dataset.tag_encoder.classes_.shape[0]\n",
        "\n",
        "        #Load the finetuned model\n",
        "        self.ner_modeler = NER_Model()\n",
        "        self.ner_modeler.build_model(n_classes, True)\n",
        "    \n",
        "    def run_ner_on_sentence(self, sample_text):\n",
        "        #Tokenize the sample text, and get the word ids\n",
        "        encoded = self.dataset.tokenizer.encode_plus(sample_text,\n",
        "                                            add_special_tokens = True,\n",
        "                                            max_length = SEQUENCE_LENGTH,\n",
        "                                            is_split_into_words=True,\n",
        "                                            return_attention_mask=True,\n",
        "                                            padding = 'max_length',\n",
        "                                            truncation=True,return_tensors = 'np')\n",
        "        input_seq = encoded['input_ids']\n",
        "        att_mask = encoded['attention_mask']\n",
        "        word_ids = encoded.word_ids()\n",
        "\n",
        "        #Predict the classes for each token\n",
        "        sample_out = self.ner_modeler.model.predict([input_seq, att_mask])\n",
        "        sample_out = np.argmax(sample_out, axis=2)\n",
        "        word_ids = np.array(word_ids)\n",
        "        valid_sample_out = sample_out[0, word_ids!=None]\n",
        "        valid_word_ids = word_ids[word_ids!=None]\n",
        "        names = [sample_text[i] for i in valid_word_ids[valid_sample_out!=self.dataset.background_class]]\n",
        "        labels = [self.dataset.tag_encoder.inverse_transform([i])[0] for i in valid_sample_out[valid_sample_out!=self.dataset.background_class]]\n",
        "\n",
        "        #Combine the tokens and correponding labels. Output the final names and their corresponding classes\n",
        "        full_names = []\n",
        "        full_labels = []\n",
        "        prev_index = -1\n",
        "        completed = {}\n",
        "        for name, label in zip(names, labels):\n",
        "            if(name not in completed):\n",
        "                if(label[0]=='B'):\n",
        "                    full_names.append(name)\n",
        "                    full_labels.append(label[2:])\n",
        "                    prev_index += 1\n",
        "                else:\n",
        "                    full_names[prev_index] = full_names[prev_index] + ' ' + name\n",
        "                completed[name] = 1\n",
        "        return full_names, full_labels\n",
        "    \n",
        "    def run_ner(self, full_text):\n",
        "        sentences = split_sentences(sample_text)\n",
        "        names = []\n",
        "        labels = []\n",
        "        for sentence in sentences:\n",
        "            snames, slabels = self.run_ner_on_sentence(sentence.split(' '))\n",
        "            names.extend(snames)\n",
        "            labels.extend(slabels)\n",
        "\n",
        "        ner_dict = {name:label for name,label in zip(names, labels)}\n",
        "        return ner_dict"
      ],
      "metadata": {
        "id": "OlmGioyiQ-rW"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = NER_Pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3pUT8DvSVBZ",
        "outputId": "3390cca0-4963-42ff-89cd-4372f8b4c7cd"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = news.df.iloc[7000]['article']"
      ],
      "metadata": {
        "id": "SFwpNjJbSYep"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.run_ner(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eI_ffIsTBew",
        "outputId": "f7102c98-d279-4c61-91c2-03038e16bde4"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2013': 'tim',\n",
              " 'American Civil Liberties Union,': 'org',\n",
              " 'Amnesty International,': 'org',\n",
              " 'Barack': 'per',\n",
              " 'Barack Obama': 'per',\n",
              " 'Clinton': 'per',\n",
              " 'Donald Trump': 'per',\n",
              " 'Edward Snowden': 'per',\n",
              " 'Ewen MacAskill—': 'per',\n",
              " 'General Eric Holder': 'per',\n",
              " 'Guardian': 'org',\n",
              " 'Hillary Clinton': 'per',\n",
              " 'Hong Kong': 'geo',\n",
              " 'Human Right Watch,': 'org',\n",
              " 'Joseph Gordon-Levitt': 'per',\n",
              " 'May': 'tim',\n",
              " 'Obama': 'per',\n",
              " 'Oliver Stone’s': 'per',\n",
              " 'President Obama': 'per',\n",
              " 'Snowden': 'per',\n",
              " 'Snowden’s': 'per',\n",
              " 'Snowden”': 'per',\n",
              " 'Stone': 'per',\n",
              " 'Toronto International Film Festival,': 'org',\n",
              " 'Trump': 'per',\n",
              " 'Tuesday': 'tim',\n",
              " 'US': 'geo',\n",
              " 'VICE’s': 'org',\n",
              " 'Wednesday,': 'tim',\n",
              " '[Snowden]': 'per',\n",
              " 'music.”': 'per',\n",
              " 'past weekend': 'tim',\n",
              " 'three': 'tim',\n",
              " '“Mr.': 'per',\n",
              " '“Snowden”': 'per'}"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "BBF4ycL4UbfF",
        "outputId": "b6757983-ef96-4458-8396-27294e4df4cc"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Edward Snowden touched off an enormous public relations campaign on Tuesday calling for Barack Obama to grant him a presidential pardon. In an interview with Ewen MacAskill— one of The Guardian reporters who secretly met Snowden in Hong Kong three years ago in an unprecedented leak of state secrets — Snowden argued that the pardon power exists precisely for people like him. There are “things that may seem unlawful in letters on a page but when we look at them morally, when we look at them ethically,” he said, “these were vital things.” Snowden’s comments come on the eve of a “Pardon Snowden” operation by The American Civil Liberties Union, Human Right Watch, Amnesty International, and other human rights groups, as first reported by VICE’s Motherboard. On Wednesday, Oliver Stone’s new film “Snowden” starring Joseph Gordon-Levitt will also be shown in 700 theaters around the country with plans for a webinar discussion between Snowden and Stone following. This past weekend at the Toronto International Film Festival, Stone publicly called for President Obama to pardon Snowden adding that he hopes “Mr. Obama has a stroke of lightning” and realizes he has created “the most extensive invasive surveillance state that ever existed.” Both Hillary Clinton and Donald Trump have said Snowden should be punished. Trump tweeted in 2013 that Snowden is a “spy who should be executed” and Clinton said Snowden shouldn’t be allowed to return to the US “without facing the music.” Snowden and his allies are hoping that President Obama will reconsider a pardon during his final months in office while free of the normal political restraints such as the internal backlash from the intelligence community. Last year, in a response to an online petition for a presidential pardon, the Obama administration said that Snowden’s actions “had severe consequences for the security of our country and the people who work day in and day out to protect it.” Edward Snowden touched off an enormous public relations campaign on Tuesday calling for Barack Obama to grant him a presidential pardon. In an interview with Ewen MacAskill— one of The Guardian reporters who secretly met Snowden in Hong Kong three years ago in an unprecedented leak of state secrets — Snowden argued that the pardon power exists precisely for people like him. There are “things that may seem unlawful in letters on a page but when we look at them morally, when we look at them ethically,” he said, “these were vital things.” Snowden’s comments come on the eve of a “Pardon Snowden” operation by The American Civil Liberties Union, Human Right Watch, Amnesty International, and other human rights groups, as first reported by VICE’s Motherboard. On Wednesday, Oliver Stone’s new film “Snowden” starring Joseph Gordon-Levitt will also be shown in 700 theaters around the country with plans for a webinar discussion between Snowden and Stone following. This past weekend at the Toronto International Film Festival, Stone publicly called for President Obama to pardon Snowden adding that he hopes “Mr. Obama has a stroke of lightning” and realizes he has created “the most extensive invasive surveillance state that ever existed.” Both Hillary Clinton and Donald Trump have said Snowden should be punished. Trump tweeted in 2013 that Snowden is a “spy who should be executed” and Clinton said Snowden shouldn’t be allowed to return to the US “without facing the music.” Snowden and his allies are hoping that President Obama will reconsider a pardon during his final months in office while free of the normal political restraints such as the internal backlash from the intelligence community. Last year, in a response to an online petition for a presidential pardon, the Obama administration said that Snowden’s actions “had severe consequences for the security of our country and the people who work day in and day out to protect it.” But there appears to be differences of opinion within the administration. Former Attorney General Eric Holder said in May that “I think that [Snowden] actually performed a public service by raising the debate that we engaged in and by the changes that we made.” Snowden may have a chance at the presidential pardon he so desires.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhSULhVfCTTYrdxldsDLr6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}